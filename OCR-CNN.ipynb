{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A CNN-based approach for OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we're going to present a method for letter recognition using Convolutional Neural Network.\n",
    "\n",
    "It is inspired by this article [\"Python Image Recognizer with Convolutional Neural Network\"](http://www.codeastar.com/convolutional-neural-network-python/) .\n",
    "\n",
    "For this project we use EMNIST dataset from [\"the EMNIST dataset website\"](http://www.codeastar.com/convolutional-neural-network-python/) .  \n",
    "\n",
    "Emnist dataset is basically extended dataset contains black-and-white 28x28 pixel formated images of handwritten digits and letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "* Set the path for data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set the path for data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/irza/Projects/gzip/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMNIST dataset provides binary dataset, so it is still raw, therefore we need to convert it to comply the neural network input.\n",
    "\n",
    "* Declare the function to convert data form binary to csv\n",
    "  \n",
    "  Source : [\"MNIST in CSV\"](https://pjreddie.com/projects/mnist-in-csv/) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(imgf, labelf, outf, n):\n",
    "    f = open(imgf, \"rb\")\n",
    "    o = open(outf, \"w\")\n",
    "    l = open(labelf, \"rb\")\n",
    "\n",
    "    f.read(16)\n",
    "    l.read(8)\n",
    "    images = []\n",
    "\n",
    "    for i in range(n):\n",
    "        image = [ord(l.read(1))]\n",
    "        for j in range(28*28):\n",
    "            image.append(ord(f.read(1)))\n",
    "        images.append(image)\n",
    "\n",
    "    for image in images:\n",
    "        o.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
    "        \n",
    "    f.close()\n",
    "    o.close()\n",
    "    l.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert the binary data to csv\n",
    "\n",
    "  There are already emnist dataset for letters divided into training and test dataset.\n",
    "\n",
    "  Here each data set is converted to csv and saved in file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "convert(path + \"emnist-letters-train-images-idx3-ubyte\", path + \"emnist-letters-train-labels-idx1-ubyte\",\n",
    "        path + \"letter_mnist_train.csv\", 60000)\n",
    "\n",
    "#test data\n",
    "convert(path + \"emnist-letters-test-images-idx3-ubyte\", path + \"emnist-letters-test-labels-idx1-ubyte\",\n",
    "        path + \"letter_mnist_test.csv\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "df_train = pd.read_csv('/home/irza/Projects/gzip/letter_mnist_train.csv')\n",
    "df_test = pd.read_csv('/home/irza/Projects/gzip/letter_mnist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the data\n",
    "\n",
    "Here we can see the content of the training and test dataset loaded from csv file and shown as dataframe.\n",
    "\n",
    "The first column contains the label of the image, the remaining columns contain the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the label column from the pixel columns\n",
    " \n",
    " Variable name ends with x contains the pixel values.\n",
    " \n",
    " Variable name ends with y contains the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = df_train.iloc[:,1:] \n",
    "df_train_y = df_train.iloc[:,:1] \n",
    "\n",
    "df_test_x = df_test.iloc[:,1:] \n",
    "df_test_y = df_test.iloc[:,:1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some images from training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplots(1,8, figsize=(20,2))\n",
    "for i in range(0,8):\n",
    "    j = i+50\n",
    "    ax[1][i].imshow(df_train_x.values[j].reshape(28,28), cmap='gray')\n",
    "    ax[1][i].set_title(df_train_y.values[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an image for each classs from training dataset.\n",
    "\n",
    "Some letters might be capital letters, rotated or even mirrored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class = np.unique(df_train_y.iloc[:,0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,int(len(unique_class)/2), figsize=(20,4))\n",
    "\n",
    "for i in np.nditer(unique_class):\n",
    "    pixel_val = df_train.loc[df_train_y.iloc[:,0] == i].iloc[:,1:].head(1).values.reshape(28,28)\n",
    "    \n",
    "    row_plot = 0\n",
    "    col_plot = (i-1) % 13\n",
    "    \n",
    "    if i > len(unique_class)/2:\n",
    "        row_plot = 1\n",
    "    \n",
    "    axes[row_plot][col_plot].imshow(pixel_val, cmap='gray')\n",
    "    axes[row_plot][col_plot].set_title(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot n image for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 7\n",
    "\n",
    "fig, axes = plt.subplots(len(unique_class),num_sample, figsize=(10,70))\n",
    "\n",
    "for i in np.nditer(unique_class):\n",
    "    pixel_val_arr = df_train.loc[df_train_y.iloc[:,0] == i].iloc[:,1:].head(num_sample).values\n",
    "    \n",
    "    for j in range(0,num_sample):\n",
    "        pixel_val = pixel_val_arr[j].reshape(28,28)\n",
    "        axes[i-1][j].imshow(pixel_val, cmap='gray')\n",
    "        axes[i-1][j].set_title(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize images\n",
    "Images typically have a pixel value range from 0 to 255(8-bit integer).\n",
    "\n",
    "We're going to scale them to values between 0 and 1(floating-point number).\n",
    "\n",
    "This helps to reduce the computation time.\n",
    "\n",
    "Then the data will be reshaped.\n",
    "\n",
    "Reshape training X and text x to (number, height, width, channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_x = df_test_x / 255\n",
    "df_train_x = df_train_x / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reshape training x and test x into matrix \n",
    "  \n",
    "  the matrix has 4 dimension : \n",
    "     \n",
    "     * number of example\n",
    "     \n",
    "     * height\n",
    "     \n",
    "     * width \n",
    "     \n",
    "     * number of channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_train_x_28x28 = np.reshape(df_train_x.values, (df_train_x.values.shape[0], 28, 28, 1))\n",
    "arr_test_x_28x28 = np.reshape(df_test_x.values, (df_test_x.values.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* turn the label to binary class matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "arr_train_y = np_utils.to_categorical(df_train_y.iloc[:,0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* spliting training dataset into training and validation dataset \n",
    "  \n",
    "  validate size = 8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 3\n",
    "split_train_x, split_val_x, split_train_y, split_val_y, = train_test_split(arr_train_x_28x28, arr_train_y, test_size = 0.08, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(result_class_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #use Conv2D to create our first convolutional layer, with 32 filters, 5x5 filter size, \n",
    "    #input_shape = input image with (height, width, channels), activate ReLU to turn negative to zero\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(28,28,1), activation='relu'))\n",
    "    \n",
    "    #add a pooling layer for down sampling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # add another conv layer with 16 filters, 3x3 filter size, \n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "    \n",
    "    #set 20% of the layer's activation to zero, to void overfit\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #convert a 2D matrix in a vector\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #add fully-connected layers, and ReLU activation\n",
    "    model.add(Dense(130, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    #add a fully-connected layer with softmax function to squash values to 0...1 \n",
    "    model.add(Dense(result_class_size, activation='softmax'))   \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_model(arr_train_y.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unclear step, it supposed to be training part !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure output parameter during training\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc',factor=0.5,patience=3,min_lr=0.00001)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range \n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally\n",
    "        height_shift_range=0.1  # randomly shift images vertically\n",
    "        )\n",
    "\n",
    "datagen.fit(split_train_x)\n",
    "\n",
    "model.fit_generator(datagen.flow(split_train_x,split_train_y, batch_size=64),\n",
    "                              epochs = 10, validation_data = (split_val_x,split_val_y),\n",
    "                              verbose = 2, steps_per_epoch=700 \n",
    "                              , callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on test dataset\n",
    "\n",
    "We're going to use our trained network to make predictions on the test dataset.\n",
    "\n",
    "The result, our predictions, will be saved as a csv-file. The files consists of the two columns ID and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_classes(arr_test_x_28x28, verbose=0)\n",
    "data_to_submit = pd.DataFrame({\"ImageId\": list(range(1,len(prediction)+1)), \"Label\": prediction})\n",
    "data_to_submit.to_csv(path + \"result.csv\", header=True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show prediction result\n",
    "\n",
    "We'd now like to show some of the predictions.\n",
    "\n",
    "Are you able to label them correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "#pick 10 images from testing data set\n",
    "start_idx = randrange(df_test_x.shape[0]-10) \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,5, figsize=(15,8))\n",
    "for j in range(0,2): \n",
    "  for i in range(0,5):\n",
    "     ax[j][i].imshow(df_test_x.values[start_idx].reshape(28,28), cmap='gray')\n",
    "     ax[j][i].set_title(\"Index:{} \\nPrediction:{}\".format(start_idx, prediction[start_idx]))\n",
    "     start_idx +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix for evaluation\n",
    "\n",
    "To get a metric or let's say a numeric value in order to evaluate how good our network has performed, we're going to calculate the confusion matrix.\n",
    "\n",
    "The confusion matrix tells us how many predictions were correct. In addition, we also know which classes(letters) were predicted instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "   \n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(df_test_y,data_to_submit.iloc[:,1] )\n",
    "np.savetxt(\"cm.csv\",conf,delimiter=\",\",fmt=\"%10i\")\n",
    "#calculated accuracy from cm\n",
    "np.round(np.trace(conf) / len(df_test_y),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load weights to JSON-file\n",
    "\n",
    "To save our trained model we're going to save the weights as a JSON-file.\n",
    "\n",
    "Once our network performs well and we saved our weights, we no longer need to train, configure or evaluate our network.\n",
    "\n",
    "We can now use this black-box to make new predictions in a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####source: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "\n",
    "#serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "#serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(split_train_x, split_train_y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
